{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "774f1f3d-5353-4a9f-858c-0cbd993577f8",
    "_uuid": "0d30485734f2e1df31e62309ed20446b987dfea8"
   },
   "source": [
    "# Задание к модулю \"RL в ИБ\"\n",
    "Есть CSV-файл c ежедневным количеством задач, которое должны обработать аналитики SOC. Каждый день можно вызывать на работу разное количество аналитиков. Если их окажется меньше, чем задач - это плохо, потому что не все задачи будут обработаны. Если больше - тоже плохо, потому что придётся заплатить им больше, чем можно было бы. Ваша задача - с применением методов RL разработать модель, которая будет предсказывать оптимальные действия на каждый день - \"вызывать\" или \"отпускать\" аналитиков.\n",
    "\n",
    "Среда, в которой будет действовать модель, уже описана. Также подготовлена \"глупая и медленная\" модель, которую (как минимум) нужно будет превзойти.\n",
    "\n",
    "Студенту предлагается реализовать метод обучения агента и его архитектуру (задания отмечены #TODO). Один из вариантов порядка работы алгоритма описан в виде комментариев на русском языке. Можно их не придерживаться и реализовать свой вариант модели RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт модулей, загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "936eed8cc53b525db200513139019e94e0a89757"
   },
   "outputs": [],
   "source": [
    "import numpy as np # линейная алгебра\n",
    "import pandas as pd # обработка данных, чтение-запись\n",
    "import time # оценка времени\n",
    "import copy # дублирование ссылочных объектов\n",
    "import chainer # нейронные сети\n",
    "import chainer.functions\n",
    "import chainer.links\n",
    "from plotly.offline import init_notebook_mode, iplot, iplot_mpl # использование JavaScript в Jupyter-ноутбуке\n",
    "init_notebook_mode()\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если какого-то модуля не хватает:\n",
    "#!pip install <module>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d5e73df952e3865515ba33fffabfea59b52c7e9"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('soc_workers.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date']) # преобразование типа\n",
    "data = data.set_index('Date') # индексация массива будет по полю \"дата\"\n",
    "print(data.index.min(), data.index.max()) # определение границ временного интервала\n",
    "data.head() # первые 5 строк для визуальной оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10fddbff0b397113861a9a3c628c98773d3bbdd6"
   },
   "outputs": [],
   "source": [
    "date_split = '2016-01-01' # до этого дня - тренировочная выборка, после - тестовая\n",
    "train = data[:date_split] # тренировочные данные\n",
    "test = data[date_split:] # тестовые данные\n",
    "len(train), len(test) # посмотрим, сколько данных в каждой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots()\n",
    "fig.append_trace(go.Scatter(x=train.index, y=train['Quantity'], mode='lines', name='train', line_color='blue'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=test.index, y=test['Quantity'], mode='lines', name='test', line_color='red'), row=1, col=1)\n",
    "fig.add_vline(x=date_split)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание среды для работы агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Среда для обучения агента повторению графика\n",
    "    \"\"\"\n",
    "    def __init__(self, data, history_t=90):\n",
    "        \"\"\"\n",
    "        Создание среды обучения из исходных данных. Среда возвращается очищенная\n",
    "        Parameters\n",
    "            data: список чисел, каждое из которых - потребность в аналитиках\n",
    "            history_t: количество позиций, которое среда возвращает в виде состояния\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.history_t = history_t # память среды в количестве позиций назад\n",
    "        self.one_step_move = 10 # смещение при шаге (найм или отпускание аналитиков)\n",
    "        self.reset() # сброс состояния среды\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Сброс данных окружения\n",
    "        Returns:\n",
    "            obs: наблюдаемое состояние\n",
    "        \"\"\"\n",
    "        self.t = 0\n",
    "        self.done = False # показатель доигранности эпизода\n",
    "        self.workers = self.data.iloc[0, :]['Quantity'] # количество нанятых аналитиков в начальный момент идеально\n",
    "        self.history = [0 for _ in range(self.history_t)] # история изменений цен акций за заданный промежуток времени\n",
    "        obs = [self.workers] + self.history # состояние\n",
    "        return obs\n",
    "    \n",
    "    def step(self, act):\n",
    "        \"\"\"\n",
    "        Переход из текущего состояние в следующее заданным действием\n",
    "        Parameters\n",
    "            act: выбранное действие: 0: stay, 1: buy, 2: sell\n",
    "        Returns\n",
    "            obs: новое состояние\n",
    "            reward: полученная за действие награда\n",
    "            done: показатель завершения эпизода\n",
    "        \"\"\"\n",
    "        reward = 0 # награда за переход\n",
    "        \n",
    "        if act == 1: # действие - нанимать аналитиков\n",
    "            self.workers += self.one_step_move\n",
    "        elif act == 2: # действие - отпускать аналитиков\n",
    "            self.workers = abs(self.workers - self.one_step_move) # без отпускания в минус \n",
    "        \n",
    "        quantity_t = self.data.iloc[self.t, :]['Quantity'] # потребность в аналитиках на этом шаге\n",
    "        quantity_pre_t = self.data.iloc[self.t - 1, :]['Quantity'] # потребность в аналитиках на предыдущем шаге\n",
    "        reward = -abs(quantity_t - self.workers) # отличие требуемого количества аналитиков от реального\n",
    "        \n",
    "        self.t += 1 # переход к следующему временному отрезку\n",
    "        self.history.pop(0) # удаление самого старого элемента истории\n",
    "        # self.history.append(quantity_t - quantity_pre_t) # запись в историю изменения потребности в аналитиках за день\n",
    "        self.history.append(quantity_t) # запись в историю потребности в аналитиках за день\n",
    "        \n",
    "        obs = [self.workers] + self.history # состояние, в которое перешёл агент\n",
    "        return obs, reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0fa94631ffc3408de7c1af0802283c32cda036a6"
   },
   "outputs": [],
   "source": [
    "env = Environment(train, history_t=90) # создаём новую среду с тренировочными данными\n",
    "print(env.reset()) # сбрасываем состояние среды и смотрим обновлённое состояние\n",
    "for _ in range(3):\n",
    "    print(env.step(np.random.randint(3))) # делаем случайное действие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и обучение агента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание архитектуры сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Network(chainer.Chain):\n",
    "    \"\"\"\n",
    "    Архитектура модели \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Инициализация модели\n",
    "        Parameters\n",
    "            input_size: количество нейронов во входном слое (количество измерений состояния)\n",
    "            hidden_size: количество нейронов в скрытом слое\n",
    "            output_size: количество выходных нейронов (количество состояний)\n",
    "        \"\"\"\n",
    "        super(Q_Network, self).__init__(\n",
    "            #TODO\n",
    "            raise NotImplementedError\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Предсказание возможных наград\n",
    "        Parameters\n",
    "            x: текущее состояние\n",
    "        Returns\n",
    "            y: возможные награды за каждое действие из этого состояния\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        raise NotImplementedError\n",
    "        return y\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Очистка градиентов\n",
    "        \"\"\"\n",
    "        #TODO\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание метода обучения агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e34a19d54e2888d5cb618e5b7e02b0a0fc633bd"
   },
   "outputs": [],
   "source": [
    "# DQN - Deep Q-Learning\n",
    "\n",
    "def train_dqn(env, Q, epoch_num=50):\n",
    "    \"\"\"\n",
    "    Обучение агента Deep Q-Learning\n",
    "    Parameters\n",
    "        env: окружение, в котором агент будет обучаться\n",
    "        Q: архитектура модели\n",
    "        epoch_num: количество эпох, которые следует учить агента, default=50\n",
    "    Returns\n",
    "        Q: обученная модель\n",
    "        total_losses: потери за каждый эпизод\n",
    "        total_rewards: награды за каждый эпизод\n",
    "    \"\"\"\n",
    "\n",
    "    Q_ast = copy.deepcopy(Q) # \"старый\" агент (предыдущее состояние таблички)\n",
    "    optimizer = chainer.optimizers.Adam() # выбор оптимизатора - Adam (для изменения скорости обучения)\n",
    "    optimizer.setup(Q) # добавление модели для отслеживания оптимизатором\n",
    "\n",
    "    epoch_num = epoch_num # количество эпох обучения агента\n",
    "    step_max = len(env.data) - 1 # максимальное количество шагов в одному эпизоде\n",
    "    memory_size = 200 # максимальный размер памяти\n",
    "    batch_size = 20 # количество эпизодов в одном батче\n",
    "    epsilon = 1.0 # вероятность, с которой будет выбираться случайное действие\n",
    "    epsilon_decrease = 1e-3 # значение, на которое каждый шаг уменьшаеття вероятность выбора каждого действия\n",
    "    epsilon_min = 0.1 # минимальное значение вероятности выбрать случайное действие\n",
    "    start_reduce_epsilon = 200 # номер шага, после которых надо уменьшать вероятность выбора случайного действия\n",
    "    train_freq = 10 # на каком шаге учиться (сколько шагов пропускать для накопления информации)\n",
    "    update_q_freq = 20 # частота обновления модели (Q-таблицы)\n",
    "    gamma = 0.97 # коэффициент дисконтирования перспективных (возможных) наград\n",
    "    show_log_freq = 5 # частота отображения информации об обучении модели для пользователя\n",
    "\n",
    "    memory = [] # память шагов\n",
    "    total_step = 0 # общее количество сделанных шагов\n",
    "    total_rewards = [] # история наград за каждый эпизод\n",
    "    total_losses = [] # история потерь за каждый эпизод\n",
    "\n",
    "    start = time.time() # время запуска обучения\n",
    "    print('\\t'.join(map(str, ['epoch', 'epsilon', 'total_step', 'log_reward', 'log_loss', 'elapsed_time'])))\n",
    "    for epoch in range(epoch_num): # для каждой эпохи\n",
    "        #TODO\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # сбрасываем состояние окружения\n",
    "        step = 0 # номер шага в эпохе\n",
    "        done = False # критерий завершения\n",
    "        total_reward = 0 # общая награда за эпоху\n",
    "        total_loss = 0 # общее значние функции потерь за эпоху\n",
    "\n",
    "        while not done and step < step_max: # пока эпизод не доигран и не превышено максимальное число шагов\n",
    "            \n",
    "            # вероятностно выбираем или случайное действие, или оптимальное (из текущей позиции)\n",
    "            #TODO\n",
    "            obs, reward, done = env.step(action) # агент делает действие и получает ответ от среды\n",
    "\n",
    "            # сохраняем в память новое состояние, действие, старое состояние, показатель завершения эпизода\n",
    "            memory.append((pobs, pact, reward, obs, done))\n",
    "            # если память заполнена, удаляем самый старый элемент\n",
    "            #TODO\n",
    "            \n",
    "            # если наиграли достаточно шагов, чтобы память была заполнена\n",
    "                # если накопили достаточно шагов, чтобы учиться\n",
    "                    # перестановка в случайном порядке\n",
    "                    # перечисление от 0 до объёма памяти\n",
    "                    # делим на группы (батчи) всю перемешанную память и для каждого\n",
    "                        # создание массива из батча\n",
    "                        # состояния\n",
    "                        # действие\n",
    "                        # награды\n",
    "                        # old состояние\n",
    "                        # показатели завершённости\n",
    "\n",
    "                        q = Q(<состояния>) # предсказание возможных наград за каждый элемент из батча новой моделью\n",
    "                        # предсказание максимальных наград старой моделью\n",
    "                        # предсказания - преобразование к ndarray и копирование значений\n",
    "                        # для каждого элемента в рассматриваемом батче\n",
    "                            # награда на этом шаге + дисконтированная будущая награда (если не конец эпизода)\n",
    "                        # сброс состояния\n",
    "                        # функция ошибки как СКО вероятных наград, предсказанных новой и старой моделями\n",
    "                        # добавляем значение функции ошибки батча к общему\n",
    "                        # обратный проход нейронной сети с заданной ошибкой\n",
    "                        # переход к следующему шагу оптимизатора\n",
    "\n",
    "                # если пора менять Q-таблицу\n",
    "                # старая модель \"догоняет\" новую\n",
    "\n",
    "            # если вероятность выбора случайного действия больше минимальной и пора её уменьшать (первые шаги сделаны)\n",
    "                # уменьшаем вероятность выбора случайного действия на заданное значение\n",
    "\n",
    "            # награду за шаг прибавляем к общей награде эпизода\n",
    "            # переходим к следующему состоянию (следующее состояние становится текущим)\n",
    "            # увеличиваем количество шагов в этом эпизоде\n",
    "            # увеличиваем общее количество шагов \n",
    "\n",
    "        # запоминаем награду за эпизод в историю\n",
    "        # запоминаем значение суммы функции ошибки за эпизод в историю\n",
    "\n",
    "        if (epoch + 1) % show_log_freq == 0: # если пора логировать успехи\n",
    "            log_reward = sum(total_rewards[((epoch + 1) - show_log_freq):]) / show_log_freq # сумма наград за последние шаги\n",
    "            log_loss = sum(total_losses[((epoch + 1) - show_log_freq):]) / show_log_freq # сумма потерь за последние шаги\n",
    "            elapsed_time = time.time() - start # время, прошедшее с последней записи в лог\n",
    "            print('\\t'.join(map(str, [epoch + 1, epsilon, total_step, log_reward, log_loss, elapsed_time]))) # лог в консоль\n",
    "            start = time.time() # обновляем значение времени\n",
    "            \n",
    "    return Q, total_losses, total_rewards # модель, потери и награды за каждый эпизод\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание агента (выбор гиперпараметров)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация модели\n",
    "#TODO\n",
    "raise NotImplementedError\n",
    "Q = Q_Network(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение агента (длительная операция)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "09836d3d2f07a0edd74224a6bcd01192cf84caa6"
   },
   "outputs": [],
   "source": [
    "# обучение модели\n",
    "Q, total_losses, total_rewards = train_dqn(Environment(train), Q, epoch_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оценка результатов обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73e0c0791daf22632f473ef371d2d62719331a2b"
   },
   "outputs": [],
   "source": [
    "def plot_loss_reward(total_losses, total_rewards):\n",
    "    \"\"\"\n",
    "    Отображение графиков обучения\n",
    "    Parameters\n",
    "        total_losses: потери за каждый эпизод\n",
    "        total_rewards: награды за каждый эпизод\n",
    "    \"\"\"\n",
    "    figure = make_subplots (rows=1, cols=2, subplot_titles=('loss', 'reward'), print_grid=False) # два окна графиков\n",
    "    figure.append_trace(go.Scatter(y=total_losses, mode='lines', line=dict(color='skyblue')), 1, 1) # потери по эпизодам\n",
    "    figure.append_trace(go.Scatter(y=total_rewards, mode='lines', line=dict(color='orange')), 1, 2) # награды по эпизодам\n",
    "    figure['layout']['xaxis1'].update(title='epoch') # подпись горизонтальный оси\n",
    "    figure['layout']['xaxis2'].update(title='epoch')\n",
    "    figure['layout'].update(height=400, width=900, showlegend=False) # установка размеров графика\n",
    "    iplot(figure) # рисование графика с применением JavaScript\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "13cc8986e9d721d143c2a5a488865e2a27c882ec"
   },
   "outputs": [],
   "source": [
    "plot_loss_reward(total_losses, total_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Настоящая\" работа агента: отпустим его зарабатывать деньги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с использованием обученного агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(env, Q):\n",
    "    \"\"\"\n",
    "    Применение обученной модели в среде\n",
    "    Parameters:\n",
    "        env: окружение, в котором будет работать агент\n",
    "        Q: обученная модель\n",
    "    Returns:\n",
    "        workers_history: история реального количества работников каждый день\n",
    "        rewards: сумма наград (правильных решений) за все эпизоды\n",
    "        acts: история всех действий агента\n",
    "    \"\"\"\n",
    "    workers_history = []\n",
    "    pobs = env.reset() # сброс параметров среды\n",
    "    acts = [] # действия агента за все эпизоды\n",
    "    rewards = [] # награды агнента за все эпизоды\n",
    "    for _ in range(len(env.data) - 1): # для каждого известного эпизода в данных\n",
    "        pact = Q(np.array(pobs, dtype=np.float32).reshape(1, -1)) # вероятные награды для каждого действия\n",
    "        pact = np.argmax(pact.data) # оптимальное действие по максимуму вероятной награды\n",
    "        acts.append(pact) # запоминаем оптимальное действие\n",
    "        obs, reward, done = env.step(pact) # переход к следующему состоянию\n",
    "        rewards.append(reward) # запоминаем награды\n",
    "        pobs = obs # следующее состояние становится текущим\n",
    "        workers_history.append(obs[0]) # запоминаем реальное количество работников\n",
    "        \n",
    "    return workers_history, rewards, acts # заработок, награды, действия агента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа с использованием детерминированного \"догоняющего\" агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stupid_slow_model(env):\n",
    "    \"\"\"\n",
    "    Простая детерминированная модель. Работает \"вдогонку\" - сравнивает текущее значение\n",
    "        с устаревшим на 30 шагов (месяц) и принимает решение\n",
    "    Parameters\n",
    "        env: окружение, в котором будет работать агент\n",
    "    Returns:\n",
    "        workers_history: история реального количества работников каждый день\n",
    "        rewards: сумма наград (правильных решений) за все эпизоды\n",
    "        acts: история всех действий агента\n",
    "    \"\"\"\n",
    "    workers_history = []\n",
    "    pobs = env.reset() # сброс параметров среды\n",
    "    acts = [] # действия агента за все эпизоды\n",
    "    rewards = [] # награды агнента за все эпизоды\n",
    "    for _ in range(len(env.data) - 1): # для каждого известного эпизода в данных\n",
    "        if pobs[0] < pobs[-30]: # если был недостаток\n",
    "            pact = 1 # нанимать\n",
    "        else: # если были лишние\n",
    "            pact = 2 # отпускать\n",
    "        acts.append(pact) # запоминаем оптимальное действие\n",
    "        obs, reward, done = env.step(pact) # переход к следующему состоянию\n",
    "        rewards.append(reward) # запоминаем награды\n",
    "        pobs = obs # следующее состояние становится текущим\n",
    "        workers_history.append(obs[0]) # запоминаем реальное количество работников\n",
    "        \n",
    "    return workers_history, rewards, acts # заработок, награды, действия агента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание сред (работа на известных данных и на тех, которых он раньше не видел)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = Environment(train)\n",
    "test_env = Environment(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Само применение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# известные данные\n",
    "stupid_train_workers_history, stupid_train_rewards, stupid_train_acts = run_stupid_slow_model(train_env)\n",
    "rl_train_workers_history, rl_train_rewards, rl_train_acts = run_model(train_env, Q)\n",
    "# неизвестные данные\n",
    "stupid_test_workers_history, stupid_test_rewards, stupid_test_acts = run_stupid_slow_model(test_env)\n",
    "rl_test_workers_history, rl_test_rewards, rl_test_acts = run_model(test_env, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рисование графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отображение данных на графике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots()\n",
    "print('Сумма наград за все тренировочные эпизоды:\\n\\tRL', int(sum(rl_train_rewards)),\n",
    "      '\\n\\tstupid:', int(sum(stupid_train_rewards)))\n",
    "print('Сумма наград за все тренировочные эпизоды:\\n\\tRL', int(sum(rl_test_rewards)),\n",
    "      '\\n\\tstupid:', int(sum(stupid_test_rewards)))\n",
    "\n",
    "fig.append_trace(go.Scatter(x=train.index, y=train['Quantity'], mode='lines', name='Потребность', line_color='green'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=train.index, y=stupid_train_workers_history, mode='lines', name='Наличие (stupid)', line_color='black'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=train.index, y=rl_train_workers_history, mode='lines', name='Наличие (RL)', line_color='red'), row=1, col=1)\n",
    "\n",
    "fig.append_trace(go.Scatter(x=test.index, y=test['Quantity'], mode='lines', name='Потребность', line_color='green'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=test.index, y=stupid_test_workers_history, mode='lines', name='Наличие (stupid)', line_color='black'), row=1, col=1)\n",
    "fig.append_trace(go.Scatter(x=test.index, y=rl_test_workers_history, mode='lines', name='Наличие (RL)', line_color='red'), row=1, col=1)\n",
    "\n",
    "fig.add_vline(x=date_split)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
